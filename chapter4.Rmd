---
title: "Analysis of Boston data"
author: "Jussi Jaatinen"
output: html_document
---

# Analysis of Boston data

### Overview of data

Boston data is included to R-package as a demonstartion or example.

describe the dataset briefly.


```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}
library(MASS)
library(dplyr)
library(plotly)
library(corrplot)
library(ggplot2)
library(GGally)
# load the data
data("Boston")
```

##### Structure and the dimensions of the data 


```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}
str(Boston)
dim(Boston)

```

##### Summary of variables

show summaries of the variables in the data.

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}
summary(Boston)
```

##### Graphical presentation of data using pairs

Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them.

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}
# plot matrix of the variables

# pairs(Boston)
ggpairs(Boston, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))
```
##### Correlations between variable

Show a graphical overview of the data and 
Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them.

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```

***
## Standardization

In standardization means of all variables are in zero. That is, variables have distributed around zero. This can be seen in summary table.

Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)


```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}
# Standardization of the dataset

boston_scaled <- scale(Boston)
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# Categoricazation of the variable of the crime rate in the scaled Boston dataset using quantiles as break points.As result we have a four-step categorial variable (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set.
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)


n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
# test <- dplyr::select(test, -crime)


```
Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot.
  

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

# Fit the linear discriminant analysis on the train set.
# Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables.
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)
# Draw the LDA (bi)plot.
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

 Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. 

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

# Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. 
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# Then predict the classes with the LDA model on the test data.
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)
# cross tabulate the results
tab <- table(correct = correct_classes, predicted = lda.pred$class) %>% addmargins()
tab
correct_pred <- tab[1,1] + tab[2,2] + tab[3,3] + tab[4,4]
incorrect_pred <- tab[5,5] - (tab[1,1] + tab[2,2] + tab[3,3] + tab[4,4]) 
pred_error <- incorrect_pred / tab[5,5]
pred_error


```

Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)  

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}
# center and standardize variables

data(Boston)

# Reload the Boston dataset and standardize the dataset.
boston_scaled <- scale(Boston)
# class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)

# Calculate the distances between the observations.
# euclidean distance matrix
dist_eu <- dist(boston_scaled)
# look at the summary of the distances
# summary(dist_eu)
# manhattan distance matrix
# dist_man <- dist(Boston, method = 'manhattan')
# look at the summary of the distances
# summary(dist_man)

# Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results.
# k-means clustering. Centers = number of clusters
km <-kmeans(boston_scaled, centers = 3)
# plot the Boston dataset with clusters
# pairs(Boston[6:10], col = km$cluster)


# Investigate what is the optimal number of clusters
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)
# plot the Boston dataset with clusters
# pairs(boston_scaled, col = km$cluster)
ggpairs(boston_scaled, mapping = aes(col = as.factor(km$cluster), alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

```

Bonus: Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influencial linear separators for the clusters? 

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

```

Super-Bonus: 

Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities? 

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color='classes')

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color='km$cluster')

```




```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

```




```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

```




```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

```











