---
title: "PCA analysis of human data"
author: "Jussi Jaatinen"
output: html_document
fig_caption: true
fig_width: 15
fig_height: 8
code_folding: hide
---

# PCA analysis of human data 

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

library(ggplot2)
library(GGally)
library(FactoMineR)
library(tidyr)

```

### Overview of data

Data for this analysis is from the United Nations Development Programme. Two datasets have been combined, Human Development Index (HDI) and Gender Inequality Index (GII). As a result, the new dataset includes following variables:

- Life_Exp     = Expected lifetime in years
- Edu_Exp      = Expected education time in years
- GNI          = Gross National Income
- Mat_Mortal   = Maternal mortality per 1000 
- Teen_Preg    = Teen pregnancy per 1000 
- F_Rep_Parl   = Female members in parliament (%)
- Edu_Rto_FM   = Ratio between females and males in education
- Work_Rto_FM  = Ratio between females and males in labour

Most of variables are to some extend skewed. In correlation matrix high positive and negative correlations can be throughout seen. Actually, most of the variables of this dataset correlated strongly. Only F_Rep_Parl and Work_Rto_FM had weak correlations to all other variables (except each other).

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

library(ggplot2)
library(GGally)
library(FactoMineR)
library(tidyr)
library(corrplot)

human <- read.table(file = "~/Documents/GitHub/IODS-project/data/human.txt", sep = "")
summary(human)
ggpairs(human, mapping = aes(corSize = 8), lower = list(combo = wrap("facethist", bins = 100)))


cor_matrix<-cor(human) %>% round(digits = 2)
# visualize the correlation matrix
corrplot.mixed(cor_matrix)

```

### Principal component analysis (PCA)

In principal component analysis the original multidimensional data is transformed to new space. Most of the cases the number of dimensions has been  diminished. The new dimensions are called principal components, which are uncorrelated to each other. The first principal component explains maximum amount of variance in original data. The second component is orthogonal to the first and explains maximum variance left. This principle continues similarly with next components and each of them are less important compared to the previous one. If eigenvalue of the component is smaller than one, it usually can be left out. In biplots, PC1 is usually visualized on x-axis and PC2 in y-axis. Arrows and labels show connections between the original features and the principal components (usually PC1 and PC2).


In biplots angles between arrows correspond correlations of the original features. The smaller the angle, the higher is positive correlation. If two arrows are parallel but point to opposite directions, it indicates high negative correlation. 90 degree's angle means zero correlation (orthogonal = compare to x-axis (PC1) and y-axis (PC2)). The length of the arrows shows the standard deviations of the original features.


```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}


# perform principal component analysis (with the SVD method)
# pca_human <- prcomp(human)

# draw a biplot of the principal component representation and the original variables
# biplot(pca_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"),xlim = c(-0.5,0.5), ylim = c(-0.5,0.5))

summary(human)

# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)
# summary(pca_human_std)
# pca2_human <- PCA(human)
# summary(pca2_human)
# draw a biplot of the principal component representation and the original variables
# biplot(pca_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"),xlim = c(-0.5,0.5), ylim = c(-0.5,0.5))
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2], xlim = c(-0.5,0.5), ylim = c(-0.5,0.5))




```

### Standardization of data and PCA

Due to PCA is sensitive to the scaling, large variance of the original feature get greater importance compared to a feature with smaller variance.
Standardization of the original data equalizes importance between original fatures. 

In following table and biplot below, PCA analysis itself is the same as above, but original data has been standardized before calculating PCA.

Variable GNI has extreme values compared to the all other variables. If data is not standardized, it change results significantly. As seen in table, in original data, importance of PC1 is 100%, due to GNI. When data is standardized, the values of GNI are closer to other variables and there are more explanatory variables, which have significance. Now, PC1 explains 53% of variance and PC2 16%. 

It must be mentioned, if you do PCA analysis with PCA function (FactoMineR) instead of prcomp (used in this analysis), it gives similar importance for PCs whether data is standardized or not.

```{r echo=FALSE, message=FALSE, warning=FALSE, comment='', out.width = "200%", out}

# standardize the variables
human_std <- scale(human)
summary(human_std)

# However, eigenvalues remain similar, whether data is standardized or not.

# perform principal component analysis (with the SVD method)
pca_human_std <- prcomp(human_std)
# summary(pca_human_std)
# pca2_human_std <- PCA(human_std)
# summary(pca2_human_std)
# draw a biplot of the principal component representation and the original variables
# biplot(pca_human_std, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"),xlim = c(-0.3,0.3), ylim = c(-0.3,0.3))
# create and print out a summary of pca_human
s <- summary(pca_human_std)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human_std, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2], xlim = c(-0.3,0.3), ylim = c(-0.3,0.3))



```

### Interpretations of PC1 and PC2

PC1 includes most of the highly correlated variables together and could be interepretated as "goodness" of life. Bigger Gross Nation Income, longer life expectancy, longer education and Ratio between females and males in education (closer to 1) all can be expected to make life and society better. In the same component Maternal mortality and Teen pregnancy correlate negatively and make life worse. Component PC2 could explain equality between genders in a society. If Female members in parliament (%) is bigger, it usually mean that Ratio between females and males in labour is also closer to 1 and women are more equal members of the society.

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

```

### Tea

Tea data is included to R package and is a questionnaire on tea. 300 participant were asked how they drink tea, what are their product's perception  and some personal details. There are totally 36 variables, of which 6 are included into this dataset.

Categorical variables (factor) are:

- Tea        (Black, Earl Grey, Green)
- How        (Alone, Lemon, Milk)
- how        (Tea bag, Tea bag + unpackaged, Unpackaged)
- sugar      (No sugar, Sugar)
- where      (Chain store, Chain store + tea shop, Tea shop)
- lunch      (Lunch, Not lunch)

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

library(ggplot2)
library(GGally)
library(FactoMineR)
library(tidyr)
library(dplyr)
data(tea)
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))
# summary(tea_time)
# str(tea_time)
# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))

```

### Multiple Correspondence Analysis on the tea data

In MCA plots categorical variables are grouped by correlations to each other or components. For example in biplot below can be seen that unpackaged tea is usually drunk in tea shops, whereas tea bag is mostly used in chain stores. Actually in both dimensions variables "how" (Tea bag, Tea bag + unpackaged, Unpackaged) and "where" (Chain store, Chain store + tea shop, Tea shop) were the most significant variables.

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=''}

mca <- MCA(tea_time, graph = FALSE)
# summary of the model
summary(mca)
# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")

```





